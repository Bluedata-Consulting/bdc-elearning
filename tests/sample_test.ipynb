{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pilothub.withopenai import ContentClient\n",
    "from pilothub.pptnotes import PPTx2Notes\n",
    "\n",
    "model = \"gpt-3.5-turbo-1106\"\n",
    "api_key = \"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\"\n",
    "source_path = r\"C:\\Users\\Anshu Pandey\\Downloads\\PPT_001_Introduction to Big Data Ingestion.pptx\"\n",
    "dest_path = r\"PPT_pilothub.pptx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = ContentClient(openai_api_key=api_key,\n",
    "                              open_ai_model=model,\n",
    "                              max_tokens=2000,\n",
    "                              temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of slides  79\n"
     ]
    }
   ],
   "source": [
    "ppt_client = PPTx2Notes(file_path=source_path)\n",
    "print(\"Total Number of slides \",len(ppt_client.slides))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_slides_index = [0,75,76,77,78]\n",
    "skip_slides_layout = [\"CoverPage\",\"Quote Slide\",\"Agenda\",\"Section Header\",\"RunningMan-Infographic\",\n",
    "                        \"QuoteHead\"]\n",
    "ppt_client.set_skip_slides(skip_slides_index=skip_slides_index,\n",
    "                           skip_slides_layout=skip_slides_layout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate Limit Error  Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-1106 in organization org-UTPW1zcMiNHrsEBoeWR8ibdh on tokens_usage_based per min: Limit 60000, Used 50079, Requested 11189. Please try again in 1.268s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens_usage_based', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Rate Limit Error  Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-1106 in organization org-UTPW1zcMiNHrsEBoeWR8ibdh on tokens_usage_based per min: Limit 60000, Used 46646, Requested 13374. Please try again in 20ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens_usage_based', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Rate Limit Error  Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-1106 in organization org-UTPW1zcMiNHrsEBoeWR8ibdh on tokens_usage_based per min: Limit 60000, Used 45101, Requested 15404. Please try again in 504ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens_usage_based', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    }
   ],
   "source": [
    "ai_prompt_for_skip_slides = \"Write a two line description for the text provided. \\n\"\n",
    "ppt_client.write_notes_to_pptx(output_path=dest_path,\n",
    "                               content_client=openai_client,\n",
    "                               SET_AI_TEXT_FOR_SKIP_SLIDES=True,\n",
    "                               AI_PROPMT_SKIP_SLIDES=ai_prompt_for_skip_slides)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slide Index  256\n",
      "Slide Layout  CoverPage\n",
      "No Title\n",
      "Slide Text  Introduction to Big Data Ingestion\n",
      "Big Data Ingestion\n",
      "Slide Index  2140757966\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Case Study: Data Corp \n",
      "Slide Text  Case Study: Data Corp \n",
      "\n",
      "\n",
      "Company Background:\n",
      "Data Corp has been a leading player in the financial sector for over a decade. \n",
      "Serving millions of customers worldwide, they offer a range of products from savings accounts to complex financial instruments. \n",
      "With the rise of digital transactions, Data Corp has seen an exponential increase in the volume of data they handle daily.\n",
      "John Anderson\n",
      "(CDE at Data Corp )\n",
      "Slide Index  2140757967\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Case Study: Data Corp \n",
      "Slide Text  Case Study: Data Corp \n",
      "\n",
      "Current Situation & Challenges: \n",
      "As Data Corp ventured into the new age of digital finance, they faced a sudden surge in data, much of which was unstructured and spread across multiple sources like web logs, customer feedback, transaction data, and mobile app analytics. \n",
      "The sheer volume of this data made it challenging to process, analyze, and extract meaningful insights in real-time.\n",
      "Mr. John Anderson, the Chief Data Officer at Data Corp, voiced the problem succinctly, \"We are drowning in information, but starving for insights. \n",
      "The traditional data handling tools are no longer sufficient for the new age challenges.\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "John Anderson\n",
      "(CDE at Data Corp )\n",
      "Slide Index  2140757968\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Case Study: Data Corp\n",
      "Slide Text  Case Study: Data Corp\n",
      "\n",
      "The Solution – Big-data Ingestion : \n",
      "To address the challenges, Mr. Anderson initiated the integration of a Big Data Ingestion tool. This tool, equipped with modern data processing capabilities, was able to collect, integrate, and preprocess vast amounts of unstructured and structured data in real-time. Not only did it consolidate data from various sources, but it also standardized it, making it ready for analytics.\n",
      "The team, under John's leadership, also invested in training sessions for their analysts to familiarize them with the new system and maximize its potential.\n",
      "John Anderson\n",
      "(CDE at Data Corp )\n",
      "Slide Index  2140757969\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Case Study: Data Corp\n",
      "Slide Text  Case Study: Data Corp\n",
      "\n",
      "The Result: \n",
      "Data Corp saw a 70% reduction in data processing times.\n",
      "Real-time insights allowed managers and decision-makers to make data-backed choices, resulting in a 30% increase in customer satisfaction and a 20% rise in sales.\n",
      "With all data centralized and streamlined, the company could now predict market trends and customer behaviors with 90% accuracy.\n",
      "Data analysts and scientists, now freed from mundane data processing tasks, focused on strategic projects, leading to the launch of three new financial products in just six months.\n",
      "Mr. Anderson remarked, \"Big Data Ingestion didn't just solve our data problem; it transformed the way we do business. Today, we're not just reacting to market changes; we're predicting and shaping them.\"\n",
      "John Anderson\n",
      "(CDE at Data Corp )\n",
      "Slide Index  2140757970\n",
      "Slide Layout  Quote Slide\n",
      "Slide Title  \u000bThis underscores the significance of Big Data Injection for organizations across segments.\n",
      "Slide Text  Let's study more about Big-Data Injection in this Module\n",
      "\u000bThis underscores the significance of Big Data Injection for organizations across segments.\n",
      "Slide Index  324\n",
      "Slide Layout  Agenda\n",
      "Slide Title  Module Coverage\n",
      "Slide Text  Module Coverage\n",
      "• Big Data Architecture Overview\n",
      "• Sources of Big Data\n",
      "\n",
      "Big Data Architecture \n",
      "• Introduction to Data Ingestion\n",
      "• Batch vs Stream Data Ingestion\n",
      "\n",
      "Introduction to Data Ingestion\n",
      "• ETL vs ELT\n",
      "• Key Factors for Data Ingestion Pipeline\n",
      "\n",
      "Data Ingestion Pipeline\n",
      "• Case 1: E-commerce company\n",
      "• Case 2: Bank\n",
      "Applications of Data Ingestion\n",
      "Challenges in Data Ingestion\n",
      "Selecting Data Ingestion Tools\n",
      "Commonly Used Data Ingestion Tools\n",
      "Key Points to remember\n",
      "References\n",
      "Summary\n",
      "Slide Index  325\n",
      "Slide Layout  Section Header\n",
      "No Title\n",
      "Slide Text  • Big Data Architecture Overview\n",
      "• Sources of Big Data\n",
      "Big Data Architecture\n",
      "Slide Index  2140757972\n",
      "Slide Layout  QuoteHead\n",
      "Slide Title  Big Data Architecture involves organizing and integrating components, technologies, and processes for efficient collection, storage, processing, and analysis of large datasets. It ensures smooth data flow, scalability, and optimal performance, enabling organizations to leverage their data effectively.\n",
      "Slide Text  Big Data Architecture involves organizing and integrating components, technologies, and processes for efficient collection, storage, processing, and analysis of large datasets. It ensures smooth data flow, scalability, and optimal performance, enabling organizations to leverage their data effectively.\n",
      "What is Big-Data Architecture? \n",
      "Slide Index  2140758008\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Big Data Architecture \n",
      "Slide Text  Big Data Architecture \n",
      "Big data architecture forms the basis for big data analytics, serving as the comprehensive system to handle substantial data volumes for analysis, guiding data analytics efforts. It offers an environment where tools extract crucial business insights from initially intricate data. This architecture blueprint defines the structure of big data infrastructures, outlining components, data flow, and security aspects, thus acting as a reference framework for designing effective big data solutions.\n",
      "\n",
      "\n",
      "The architecture components of big data analytics typically consists of four logical layers and performs four major processes:\n",
      "Slide Index  2140758062\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Big Data Architecture \n",
      "Slide Text  Big Data Architecture \n",
      "Sources Layer\n",
      "In a big data environment, the capability exists to efficiently handle both batch processing and real-time processing of extensive data sources. These sources encompass a variety of origins, including data warehouses, relational database management systems, SaaS, and IoT devices.\n",
      "Management & Storage Layer\n",
      "This segment receives data from various sources, translates it into a format compatible with data analytics tools, and stores the processed data in the transformed format, facilitating effective analysis.\n",
      "Analysis Layer\n",
      "this layer is where various analytics tools come into play. These tools play a pivotal role in extracting pertinent business intelligence from the reservoir of data stored in the big data storage layer.\n",
      "Consumption Layer\n",
      "the consumption layer is the interface through which results obtained from the big data analysis layer are received. These results are then channeled and presented to the appropriate output layer, which is also referred to as the business intelligence layer.\n",
      "Slide Index  2140758010\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Components of Big Data Architecture\n",
      "Slide Text  Components of Big Data Architecture\n",
      "Storage: This is where the data is stored and managed. \n",
      "Slide Index  2140758025\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Components of Big Data Architecture\n",
      "Slide Text  Components of Big Data Architecture\n",
      "Processing: For transforming and analyzing the data.\n",
      "Slide Index  2140758026\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Components of Big Data Architecture\n",
      "Slide Text  Components of Big Data Architecture\n",
      "Analysis: This involves querying the data to extract meaningful insights.\n",
      "Slide Index  2140758027\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Components of Big Data Architecture\n",
      "Slide Text  Components of Big Data Architecture\n",
      "Visualization: Presenting the data in a visual format like charts, graphs, reports, etc.\n",
      "Slide Index  2140758030\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Big Data Architecture\n",
      "Slide Text  Big Data Architecture\n",
      "Data Sources\n",
      "Real Time Ingestion\n",
      "Batch \n",
      "Processing\n",
      "Orchestration\n",
      "Data Storage\n",
      "Analytical Data Store\n",
      "Stream Processing\n",
      "Analytics and Reporting\n",
      "Slide Index  2140758004\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Sources of Big Data Generation\n",
      "Slide Text  Sources of Big Data Generation\n",
      "Web & social media \n",
      "The largest sources of Big Data. It includes clicks, logs, posts, tweets, etc. Every interaction of users with the internet, be it clicking on a link, liking a post, sharing a tweet, etc. generates data. Social media platforms like Facebook, Twitter, Instagram, etc., generate a huge amount of data daily.\n",
      "Clicks\n",
      "Log\n",
      "Posts\n",
      "tweets\n",
      "Slide Index  2140758032\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Sources of Big Data Generation\n",
      "Slide Text  Sources of Big Data Generation\n",
      "Mobile devices \n",
      " Mobile devices generate data from apps, GPS, sensors, etc. Every app that is used, every location that is accessed via GPS, every activity that is recorded by the sensors in the mobile devices generates data.\n",
      "Apps\n",
      "GPS\n",
      "Sensors\n",
      "Slide Index  2140758033\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Sources of Big Data Generation\n",
      "Slide Text  Sources of Big Data Generation\n",
      "Internet of Things \n",
      "IoT devices include sensors, RFID tags, and other connected devices. These devices continuously generate data about their status, location, and other parameters. For example, a smart thermostat will generate data about the temperature, humidity, etc.\n",
      "Sensors\n",
      "RFID\n",
      "Devices\n",
      "Slide Index  2140758034\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Sources of Big Data Generation\n",
      "Slide Text  Sources of Big Data Generation\n",
      "Business Systems  \n",
      "Business systems like Enterprise Resource Planning (ERP), Customer Relationship Management (CRM), accounting software, etc., generate a large amount of data. These systems record every transaction, every interaction with the customers, every financial transaction, etc.\n",
      "ERP\n",
      "CRM\n",
      "Accounting\n",
      "Slide Index  2140758035\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Sources of Big Data Generation\n",
      "Slide Text  Sources of Big Data Generation\n",
      "Multimedia\n",
      "Multimedia includes images, audio, video, documents, etc. Every file that is created, every video that is recorded, every document that is written generates data.\n",
      "Images\n",
      "Audio\n",
      "Video\n",
      "Documents\n",
      "Slide Index  2140758036\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Sources of Big Data Generation\n",
      "Slide Text  Sources of Big Data Generation\n",
      "Others\n",
      "Extensive archives, transactions, and logs, etc. Sources of Big Data include archival emails, financial transaction data, server log files, and more.\n",
      "Logs\n",
      "Transactions\n",
      "Archives\n",
      "Slide Index  2140757989\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  QUIZ \n",
      "Slide Text  QUIZ \n",
      "Which of the following components are typically found in the architecture of Big Data systems?\n",
      "\n",
      "Centralized Database Management Systems\n",
      "Single-tier Architecture\n",
      "Data Warehouses\n",
      "Distributed File Systems\n",
      "\n",
      "Answer: D\n",
      "\n",
      "Explanation: \n",
      "In Big Data architectures, distributed file systems play a crucial role in handling massive volumes of data. These file systems, such as Hadoop Distributed File System (HDFS), are designed to store and manage data across a cluster of machines. Unlike traditional centralized databases, distributed file systems are scalable, fault-tolerant, and can efficiently handle the storage and processing requirements of Big Data.\n",
      "Slide Index  2140758017\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  QUIZ \n",
      "Slide Text  QUIZ \n",
      "Which of the following are common sources of Big Data generation?\n",
      "\n",
      "Printed Books\n",
      "Online Transaction Records\n",
      "Rotary Phones\n",
      "Analog Thermometers\n",
      "\n",
      "Answer: B\n",
      "\n",
      "Explanation: \n",
      "Online transaction records are one of the significant sources of Big Data generation. E-commerce platforms, financial institutions, and various online services generate massive amounts of transaction data daily.\n",
      "Slide Index  2140757996\n",
      "Slide Layout  RunningMan-Infographic\n",
      "Slide Title  In a nutshell, we learnt\n",
      "Slide Text  In a nutshell, we learnt\n",
      "\n",
      "This module has covered two key areas. Firstly, we explored Big Data architecture components, including storage, processing, analysis, and visualization tools. Our architecture diagram illustrated the flow of data.\n",
      "\n",
      "Secondly, we examined diverse sources of Big Data, ranging from web interactions and mobile activities to IoT devices and business systems. This foundation sets the stage for deeper exploration, enabling you to tap into the potential of Big Data for real-world impact.\n",
      "Slide Index  2140757985\n",
      "Slide Layout  Section Header\n",
      "No Title\n",
      "Slide Text  • Introduction to Data Ingestion\n",
      "• Batch vs Stream Data Ingestion\n",
      "\n",
      "Introduction to Data Ingestion\n",
      "Slide Index  2140757973\n",
      "Slide Layout  QuoteHead\n",
      "Slide Title  Big Data ingestion refers to the process of collecting, transferring, and loading large volumes of data from various sources into a storage or processing system, often for analysis and decision-making purposes. It involves handling data from diverse formats and sources, ensuring its accuracy, and preparing it for further processing.\n",
      "Slide Text  Big Data ingestion refers to the process of collecting, transferring, and loading large volumes of data from various sources into a storage or processing system, often for analysis and decision-making purposes. It involves handling data from diverse formats and sources, ensuring its accuracy, and preparing it for further processing.\n",
      "What is Data Ingestion? \n",
      "Slide Index  349\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Data Ingestion\n",
      "Slide Text  Data Ingestion\n",
      "What is Data Ingestion?\n",
      "Data ingestion is the crucial process of acquiring, validating, and processing data from a variety of sources.\n",
      "It serves as the foundational step within a big data pipeline, setting the stage for subsequent processing and analysis.\n",
      "Why is Data Ingestion Important?\n",
      "Data ingestion is the critical first step in the journey of transforming raw data into valuable insights.\n",
      "It ensures that data is efficiently moved from various sources into data lakes, data warehouses, or other storage systems.\n",
      "Slide Index  2140757974\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Key Functions of Data Ingestion\n",
      "Slide Text  Key Functions of Data Ingestion\n",
      "\n",
      "Acquisition\n",
      "\n",
      "Validation\n",
      "\n",
      "Processing\n",
      "\n",
      "Storage\n",
      "\n",
      "Routing\n",
      "\n",
      "Gathering data from diverse sources such as databases, APIs, logs, sensors, and more.\n",
      "Verifying the integrity, quality, and consistency of the incoming data.\n",
      " Performing initial transformations like data cleaning, enrichment, and structuring.\n",
      "Storing ingested data in appropriate formats and structures for future analysis.\n",
      "Directing data to the correct destinations within the data architecture.\n",
      "Slide Index  2140757995\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Data Ingestion Process Steps\n",
      "Slide Text  Data Ingestion Process Steps\n",
      "\n",
      "Source Identification\n",
      "Identifying the data sources that hold relevant information.\n",
      "\n",
      "\n",
      "Connection Establishment\n",
      "Creating connections or interfaces to extract data from the sources.\n",
      "\n",
      "\n",
      "Data Extraction\n",
      "Fetching data from sources using various methods (e.g., batch processing, real-time streaming).\n",
      "\n",
      "\n",
      "Data Transformation\n",
      "Converting raw data into a consistent format, handling missing values, and applying necessary encoding.\n",
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "Slide Index  2140758037\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Data Ingestion Process Steps\n",
      "Slide Text  Data Ingestion Process Steps\n",
      "\n",
      "Data Loading\n",
      "Storing transformed data in data lakes, warehouses, or other storage solutions.\n",
      "\n",
      "\n",
      "Validation and Error Handling\n",
      "Ensuring data accuracy and handling errors that may occur during the ingestion process.\n",
      "\n",
      "\n",
      "Metadata Management\n",
      "Adding metadata to the ingested data to provide context for downstream processing.\n",
      "\n",
      "\n",
      "Notification and Monitoring\n",
      "Alerting stakeholders about successful or failed data ingestion. Monitoring system performance.\n",
      "\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "Slide Index  2140758038\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Challenges in Data Ingestion\n",
      "Slide Text  Challenges in Data Ingestion\n",
      "Volume and Velocity\n",
      "\n",
      "Data Variety\n",
      "\n",
      "Data Quality\n",
      "\n",
      "Scalability\n",
      "\n",
      "Latency\n",
      "\n",
      "Slide Index  2140758039\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Batch vs Stream Data Ingestion\n",
      "Slide Text  Batch vs Stream Data Ingestion\n",
      "Batch Data Ingestion\n",
      "Stream Data Ingestion\n",
      "Latency\n",
      "Batch data ingestion has higher latency due to processing intervals.\n",
      "\n",
      "Stream data ingestion offers lower latency since data is processed immediately upon arrival\n",
      "Timeliness\n",
      "Batch data ingestion is more suited for historical analysis and reports.\n",
      "Stream data ingestion is suitable for applications requiring timely insights and actions.\n",
      "\u000b\n",
      "Complexity\n",
      "Batch data ingestion systems are generally simpler.\n",
      "Stream data ingestion systems tend to be more complex to set up due to the need for real-time processing and handling potential data spikes. \n",
      "Scalability\n",
      "Batch data ingestion may struggle with very high data volumes.\n",
      "Stream data ingestion can handle high-velocity data streams, making it well-suited for scenarios with rapidly changing data.\n",
      "Infrastructure\n",
      "Batch data ingestion typically requires tools for batch processing and data warehousing.\n",
      "Stream data ingestion often requires specialized infrastructure and tools to handle data in motion.\n",
      "Slide Index  2140758040\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Batch vs Stream Data Ingestion\n",
      "Slide Text  Batch vs Stream Data Ingestion\n",
      "Examples\n",
      "Batch Data Ingestion \n",
      "\n",
      "Daily Website Logs: \n",
      "\n",
      "\tCollecting logs from a website over a 24-hour period and processing them in a batch at the end of the day. Useful for generating daily traffic reports and analyzing trends over time.\n",
      "\n",
      "Customer Transaction Logs: \n",
      "\n",
      "\tCollecting a day's worth of customer transactions and processing them in a batch overnight. Ideal for generating financial reports and reconciling accounts\n",
      "Slide Index  2140758063\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Batch vs Stream Data Ingestion\n",
      "Slide Text  Batch vs Stream Data Ingestion\n",
      "Stream Data Ingestion\n",
      "\n",
      "User Clicks:\n",
      "\t\n",
      "\tProcessing user clicks on a website as they happen in real time. Useful for immediate analysis of user behavior and optimizing web content.\n",
      "\n",
      "IoT Sensor Data:\n",
      "\n",
      "\t Collecting data from sensors on industrial equipment as it's generated. Allows for real-time monitoring of equipment health and predicting maintenance needs.\n",
      "Examples\n",
      "Slide Index  2140757953\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  QUIZ \n",
      "Slide Text  QUIZ \n",
      "What is the primary purpose of data ingestion in the context of data processing?\n",
      "\n",
      "Data ingestion is solely responsible for data storage.\n",
      "Data ingestion involves transforming data for reporting purposes.\n",
      "Data ingestion prepares data for further processing and analysis.\n",
      "Data ingestion focuses on data visualization and dashboard creation.\n",
      "\n",
      "Answer: C\n",
      "Explanation: \n",
      "Data ingestion involves the process of collecting and importing data from various sources into a data storage system. Its primary purpose is to prepare the collected data for further processing, analysis, and storage. \n",
      "\n",
      "\n",
      "Slide Index  2140758018\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  QUIZ \n",
      "Slide Text  QUIZ \n",
      "\n",
      "What is a key difference between batch data ingestion and stream data ingestion?\n",
      "\n",
      "A. Batch ingestion processes data in fixed-size chunks and is suitable for historical data analysis.\n",
      "B. Stream ingestion processes data in real-time as it arrives.\n",
      "C. Batch ingestion is ideal for low-latency processing.\n",
      "D. Stream ingestion deals with data in large, time-bound chunks.\n",
      "\n",
      "Answer: B\n",
      "\n",
      "Explanation:\n",
      "Batch data ingestion processes data in chunks over a period, while stream data ingestion deals with data in real-time as it arrives, making it suitable for immediate processing and analysis.\n",
      "Slide Index  2140757998\n",
      "Slide Layout  RunningMan-Infographic\n",
      "Slide Title  In a nutshell, we learnt\n",
      "Slide Text  In a nutshell, we learnt\n",
      "Data ingestion involves acquiring, validating, and processing data from various sources.\n",
      "It's a critical initial step in the big data pipeline.\n",
      "Data is moved from sources into data lakes or warehouses.\n",
      "Batch ingestion handles large data volumes collected over time, loaded in batches.\n",
      "Stream ingestion deals with continuous real-time data, loaded incrementally.\n",
      "Slide Index  326\n",
      "Slide Layout  Section Header\n",
      "No Title\n",
      "Slide Text  Data Ingestion Pipeline\n",
      "ETL vs ELT\n",
      "Key Factors for Data Ingestion Pipeline\n",
      "Data Ingestion Pipeline\n",
      "Slide Index  2140758041\n",
      "Slide Layout  QuoteHead\n",
      "Slide Title  Data ingestion pipeline is a series of steps and tools used to collect, validate, transform, and load data from various sources into a database, data warehouse, or data lake. This process ensures the data is clean, consistent, and readily available for analysis or other applications.\n",
      "Slide Text  Data ingestion pipeline is a series of steps and tools used to collect, validate, transform, and load data from various sources into a database, data warehouse, or data lake. This process ensures the data is clean, consistent, and readily available for analysis or other applications.\n",
      "Data Ingestion Pipeline\n",
      "Slide Index  2140758042\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Data Ingestion Pipeline\n",
      "Slide Text  Data Ingestion Pipeline\n",
      "This process often involves several steps to move data from the source to the destination. Here are the common steps involved in a data ingestion pipeline:\n",
      "\n",
      "Metadata Collection\n",
      "Metadata Storage\n",
      "Metadata Usage\n",
      "Metadata Management\n",
      "Data Import\n",
      "Data Indexing\n",
      "Data Loading\n",
      "Data Formatting\n",
      "Data Enrichment\n",
      "Data Aggregation\n",
      "Data Transformation\n",
      "Quality Check\n",
      "Data Cleaning\n",
      "Data Validation\n",
      "Source Identification\n",
      "Data Retrieval\n",
      "Data Extraction\n",
      "Slide Index  2140758043\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Data Ingestion Pipeline\n",
      "Slide Text  Data Ingestion Pipeline\n",
      "Data Extraction: is the first step in the data ingestion pipeline\n",
      "Slide Index  2140758044\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Data Ingestion Pipeline\n",
      "Slide Text  Data Ingestion Pipeline\n",
      "Data Validation: is a critical step in the data ingestion pipeline. It involves two main sub-steps\n",
      "Slide Index  2140758045\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Data Ingestion Pipeline\n",
      "Slide Text  Data Ingestion Pipeline\n",
      "Data Transformation: the data is cleaned and converted into a format that can be easily analyzed or used by other applications. It typically involves the following sub-steps\n",
      " \n",
      "Slide Index  2140758046\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Data Ingestion Pipeline\n",
      "Slide Text  Data Ingestion Pipeline\n",
      "Data Loading: is the process of moving the prepared and transformed data into a target destination such as a database, data warehouse, or a data lake.\n",
      "Slide Index  2140758047\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Data Ingestion Pipeline\n",
      "Slide Text  Data Ingestion Pipeline\n",
      "Metadata Management: Metadata management is the process of collecting, storing, and using metadata to make data more organized, useful, and accessible. \n",
      "Metadata Collection\n",
      "\n",
      "Gathering metadata from various sources, which can include information about the data source, type, format, transformations applied, and collection time.\n",
      "Metadata Storage\n",
      "\n",
      "Storing the collected metadata in a structured and organized manner, often in a metadata repository like a database, file system, or specialized system.\n",
      "Metadata Usage\n",
      "\n",
      "Metadata usage involves utilizing the collected and stored metadata for data lineage, cataloging, governance, and integration.\n",
      "Slide Index  2140758012\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  ETL vs ELT\n",
      "Slide Text  ETL vs ELT\n",
      "Slide Index  2140758048\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  ETL vs ELT\n",
      "Slide Text  ETL vs ELT\n",
      "ETL\n",
      "ELT\n",
      "Processing \n",
      "Location\n",
      "Complexity and Processing Time\n",
      "Order of Operations\n",
      "Extract -> Transform -> Load\n",
      " Extract -> Load -> Transform\n",
      "Transformation occurs before loading, typically in a separate staging area or tool.\n",
      "Transformation occurs after loading, directly in the target data warehouse or database.\n",
      "More time-consuming and complex due to pre-loading transformation.\n",
      "Faster and less complex as transformation is done post-loading using the target system's computational power.\n",
      "Tools\n",
      "Traditional ETL tools like Informatica, Talend, SSIS.\n",
      "Modern data warehouse platforms like Amazon Redshift, Google BigQuery, Snowflake, or traditional ETL tools configured for ELT.\n",
      "Scalability\n",
      "Limited by the capacity of the transformation server/tool.\n",
      "Higher scalability leveraging the power of the target system.\n",
      "Data Storage\u000b\n",
      "Temporary storage needed for intermediate data.\n",
      " No additional storage needed as data is loaded directly into the target system.\n",
      "Flexibility\n",
      "Fixed transformation logic; changes require modifying the ETL process.\n",
      "More flexible transformation logic; can be modified or extended in the target system without changing the initial load process.\n",
      "Cost\n",
      "Additional costs for separate transformation servers/tools.\n",
      "more cost-effective as it leverages the existing computational power of the target system.\n",
      "Slide Index  2140758049\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Key Factors for Data Ingestion Pipeline\u000b\n",
      "Slide Text  Key Factors for Data Ingestion Pipeline\u000b\n",
      "Slide Index  2140758015\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Key Factors for Data Ingestion Pipeline\u000b\n",
      "Slide Text  Key Factors for Data Ingestion Pipeline\u000b\n",
      "Slide Index  2140758050\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Key Factors for Data Ingestion Pipeline\u000b\n",
      "Slide Text  Key Factors for Data Ingestion Pipeline\u000b\n",
      "Slide Index  2140757990\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  QUIZ \n",
      "Slide Text  QUIZ \n",
      "Which of the following is a key factor to consider when designing a Data Ingestion Pipeline?\n",
      "\n",
      "The color scheme used for data visualization.\n",
      "The number of data sources connected to the pipeline.\n",
      "The font style used in data reports.\n",
      "The type and volume of data to be ingested.\n",
      "\n",
      "Answer: D\n",
      "\n",
      "Explanation: \n",
      "When designing a Data Ingestion Pipeline, one of the critical factors to consider is the type of data you are dealing with (structured, semi-structured, unstructured) and the volume of data to be ingested. This information helps in selecting the appropriate tools, technologies, and strategies for efficient data ingestion and processing.\n",
      "\n",
      "Slide Index  2140758019\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  QUIZ \n",
      "Slide Text  QUIZ \n",
      "Which of the following is a common challenge in building a Data Ingestion Pipeline?\n",
      "\n",
      "Lack of data storage options\n",
      "Data security concerns\n",
      "Slow internet connection\n",
      "Incompatibility with data visualization tools\n",
      "\n",
      "Answer: B\n",
      "\n",
      "Explanation: \n",
      "Data security is a significant challenge in building a Data Ingestion Pipeline. Ensuring that data is transferred and stored securely is essential to protect sensitive information and maintain data integrity. It involves implementing encryption, access controls, and other security measures to prevent unauthorized access and data breaches.\n",
      "Slide Index  2140757997\n",
      "Slide Layout  RunningMan-Infographic\n",
      "Slide Title  In a nutshell, we learnt\n",
      "Slide Text  In a nutshell, we learnt\n",
      "Data Ingestion Pipeline is the process of collecting, transferring, and loading data from diverse sources into a storage or processing system. It's the foundation of effective data management and analysis.\n",
      "ETL involves extracting data, transforming it, and then loading it into a target system.\n",
      "ELT loads raw data into the target system first and then performs transformations.\n",
      "Building a robust data ingestion pipeline requires attention to key factors: data volume, variety, velocity, quality, scalability, and security to handle diverse data efficiently and securely.\n",
      "Slide Index  2140757987\n",
      "Slide Layout  Section Header\n",
      "No Title\n",
      "Slide Text  Use Case 1: E-commerce company\n",
      "Use Case 2: Bank\n",
      "\u000b\n",
      "Applications of Data Ingestion\n",
      "Slide Index  2140757993\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title   E-commerce company Case - 1\n",
      "Slide Text   E-commerce company Case - 1\n",
      "Flipkart\n",
      "Challenge: \n",
      "\tAs one of the leaders in the e-commerce industry, Flipkart must manage huge volumes of data pouring in from various sources like their website, mobile apps, customer reviews, inventory systems (ERP), and more. Managing and deriving insights from this complex web of data in real-time can be daunting.\n",
      "Slide Index  2140758051\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title   E-commerce company Case - 1\n",
      "Slide Text   E-commerce company Case - 1\n",
      "Solution:\n",
      "\tA scalable data ingestion pipeline was designed to channel all the raw data into Amazon S3 for durable storage. From S3, the data was then directed to Apache Spark, a fast and general-purpose cluster-computing system for big data processing. This setup allowed Flipkart to handle varying data velocities and volumes seamlessly.\n",
      "Flipkart\n",
      "Slide Index  2140758052\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title   E-commerce company Case - 1\n",
      "Slide Text   E-commerce company Case - 1\n",
      "Flipkart\n",
      "Impact:\n",
      "\t With a well-structured data ingestion pipeline in place, Flipkart was able to process data more efficiently. This led to quicker insights into customer behavior, more accurate inventory forecasting, and the ability to offer targeted product recommendations, enhancing the customer experience and potentially increasing sales.\n",
      "Slide Index  2140758053\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Banking & Finance Case - 2\n",
      "Slide Text  Banking & Finance Case - 2\n",
      "HDFC\n",
      "Challenge:\n",
      "\tHDFC, a major bank, deals with millions of transactions daily, from physical branches, ATMs, online banking, to mobile apps. Managing this diverse and high-velocity data in real-time to offer better services and ensure security is a significant challenge.\n",
      "Slide Index  2140758054\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Banking & Finance Case - 2\n",
      "Slide Text  Banking & Finance Case - 2\n",
      "HDFC\n",
      "Solution:\n",
      "\tTo handle real-time data streams, HDFC implemented a data ingestion solution with Kafka, a distributed streaming platform. This setup ensured immediate capture and relay of transaction data. For processing this vast amount of real-time data, in-memory computation was enabled using Apache Spark, allowing rapid analysis without the need to write data to disk.\n",
      "Slide Index  2140758055\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Banking & Finance Case - 2\n",
      "Slide Text  Banking & Finance Case - 2\n",
      "HDFC\n",
      "Impact:\n",
      "Fraud Detection: \n",
      "\tWith real-time data ingestion and processing, HDFC can now detect \tunusual \ttransaction patterns almost instantaneously. If a suspicious activity is detected, \tsuch as a large withdrawal or multiple transactions In a short time, alerts can be \traised, and necessary actions like \tblocking the card can be triggered\tautomatically.\n",
      "\n",
      "Personalization: \n",
      "\tHDFC can use the real-time data to understand customers' behavior \tand \tpreferences, offering tailored services or promotions. For instance, if a \tcustomer frequently transacts with a particular merchant or category, \tthe bank \tcan provide specialized offers or cashback related to those transactions. \n",
      "Slide Index  2140758020\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Quiz\n",
      "Slide Text  Quiz\n",
      "What is a common application of data ingestion in the field of data analytics and business intelligence?\n",
      "\n",
      "Data visualization\n",
      "Data encryption\n",
      "Data storage\n",
      "Data cleaning\n",
      "\n",
      "Answer: A\n",
      "\n",
      "\n",
      "Explanation: Data ingestion is the process of collecting and importing data from various sources into a storage and processing system. In the context of data analytics and business intelligence, one of the key applications of data ingestion is to prepare data for visualization. Data ingestion helps in gathering data from different sources, cleaning it, and structuring it in a way that is suitable for creating meaningful visualizations and dashboards. \n",
      "Slide Index  2140757999\n",
      "Slide Layout  RunningMan-Infographic\n",
      "Slide Title  In nutshell, we learnt\n",
      "Slide Text  In nutshell, we learnt\n",
      "An e-commerce platform like Flipkart ingests real-time user behavior and purchase data to tailor shopping experiences, optimize marketing efforts, and manage inventory efficiently.\n",
      "HDFC bank leverages real-time data ingestion from diverse transaction sources, processing it with Kafka and Spark, to enable instant fraud detection and offer personalized banking experiences to its customers.\n",
      "Slide Index  2140757988\n",
      "Slide Layout  Section Header\n",
      "No Title\n",
      "Slide Text  Selecting Data Ingestion Tools\n",
      "Commonly Used Data Ingestion Tools\n",
      "\u000b\n",
      "Challenges in Data Ingestion\n",
      "Slide Index  2140758024\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Challenges in Data Ingestion\n",
      "Slide Text  Challenges in Data Ingestion\n",
      "Volume, variety and velocity of data\n",
      "\n",
      "Network bandwidth constraints\n",
      "\n",
      "Hardware capacity\n",
      "\n",
      "Complex data formats\n",
      "\n",
      "Duplicate and inconsistent data\n",
      "\n",
      "Data security\n",
      "\n",
      "Legacy systems integration\n",
      "\n",
      "Monitoring and maintenance\n",
      "\n",
      "Slide Index  2140758056\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Challenges in Data Ingestion\n",
      "Slide Text  Challenges in Data Ingestion\n",
      "Volume, variety, and velocity of data:\n",
      "\n",
      "Handling vast amounts of data from diverse sources at high speed demands robust infrastructure and advanced processing capabilities.\n",
      "\n",
      "Network bandwidth constraints: \n",
      "\n",
      "Ingesting huge data volumes, especially in real-time, can strain network resources, leading to bottlenecks and potential data loss.\n",
      "\n",
      "Hardware capacity:  \n",
      "\n",
      "Physical limits of storage and computation can hinder the efficient ingestion and processing of large datasets, necessitating scaling or upgrades.\n",
      "\n",
      "Complex data formats:  \n",
      "\n",
      "Data from varied sources might come in multiple formats, requiring intricate parsers and transformers to make it uniform and usable.\n",
      "Slide Index  2140758064\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Challenges in Data Ingestion\n",
      "Slide Text  Challenges in Data Ingestion\n",
      "Duplicate and inconsistent data:\n",
      "\n",
      "Ingested data can often have redundancies or inconsistencies, leading to inaccurate analyses and potential business missteps.\n",
      "\n",
      "Data security: \n",
      "\n",
      "Ensuring data is protected during ingestion, from external sources, is crucial to prevent breaches and comply with privacy regulations.\n",
      "\n",
      "Legacy systems integration: \n",
      "\n",
      "Older systems might not easily integrate with modern data ingestion tools, requiring additional middleware or custom solutions.\n",
      "\n",
      "Monitoring and maintenance: \n",
      "\n",
      "Continuous oversight is essential to ensure data ingestion pipelines run smoothly, and any issues or failures are promptly addressed.\n",
      "Slide Index  317\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Selecting Data Ingestion Tools\n",
      "Slide Text  Selecting Data Ingestion Tools\n",
      "68\n",
      "Data formats supported\n",
      "Choose a tool that supports a broad range of data formats, from structured databases to unstructured logs, ensuring flexibility in ingesting diverse datasets.\n",
      "Batch vs streaming\n",
      "Depending on your data flow, opt for tools that cater to either batch processing (bulk data at intervals) or real-time streaming (continuous data flow).\n",
      "Throughput and scalability\n",
      "Select tools that can handle your current data volume and scale with future growth, ensuring uninterrupted data flow even during peak loads.\n",
      "Data validation features\n",
      "Ensure the ingestion tool offers features to validate and cleanse incoming data, guaranteeing that only quality data enters the system.\n",
      "Slide Index  2140758065\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Selecting Data Ingestion Tools\n",
      "Slide Text  Selecting Data Ingestion Tools\n",
      "Integration and plugins\n",
      "Pick tools that easily integrate with your existing infrastructure and offer plugins for various data sources, targets, and transformation tasks.\n",
      "Ease of use\n",
      "A user-friendly interface and intuitive design can simplify complex ingestion tasks, reducing the learning curve and setup time.\n",
      "Cost\n",
      "Consider the total cost of ownership, not just the initial price. Evaluate factors like licensing, infrastructure, and operational costs.\n",
      "Support and documentation\n",
      "For tools with robust documentation and active community or professional support, aiding in troubleshooting and optimizing your ingestion pipeline.\n",
      "Slide Index  2140758058\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Commonly Used Data Ingestion Tools\n",
      "Slide Text  Commonly Used Data Ingestion Tools\n",
      "Open source:\n",
      "A distributed streaming platform suitable for building real-time data pipelines and streaming applications.\n",
      "Designed to collect, aggregate, and transport large amounts of log data from various sources to a centralized data store.\n",
      "A data integration system offering visual design and real-time data flow management and transformation.\n",
      "Sqoop is a command-line tool for transferring data between relational databases and Hadoop. It can import data like MySQL into Hadoop HDFS.\n",
      "An in-memory computation engine that supports real-time data processing and analytics.\n",
      "Slide Index  2140758060\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Commonly Used Data Ingestion Tools\n",
      "Slide Text  Commonly Used Data Ingestion Tools\n",
      "Cloud-based:\n",
      "Amazon's real-time data streaming service, designed for large scale, distributed data ingestion and processing.\n",
      "Google's asynchronous messaging service that decouples services that produce events from services that process events.\n",
      "Microsoft's cloud-based ETL and data integration service that orchestrates and automates data workflows.\n",
      "Slide Index  2140758059\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Commonly Used Data Ingestion Tools\n",
      "Slide Text  Commonly Used Data Ingestion Tools\n",
      "Commercial:\n",
      "A comprehensive data integration tool offering ETL, data masking, quality, and replication.\n",
      "Provides a platform for data integration, quality, and management, with both open-source and commercial versions.\n",
      "A fully-managed data integration platform focused on automating data connectors to various sources.\n",
      "Enables building, executing, operating, and protecting batch and streaming dataflows.\n",
      "Slide Index  2140757991\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Quiz\n",
      "Slide Text  Quiz\n",
      "When choosing a tool for data ingestion, what is a primary consideration to account for?\n",
      "\n",
      "The popularity of the tool in the market.\n",
      "The graphics and visuals the tool provides.\n",
      "Compatibility with the source and destination data systems.\n",
      "The age of the tool in terms of its release date.\n",
      "\n",
      "Answer: C\n",
      "\n",
      "Explanation:\n",
      "One of the primary challenges in selecting a data ingestion tool is ensuring it is compatible with both the source (where the data is coming from) and the destination (where the data is going to) data systems.\n",
      "Slide Index  2140758061\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Quiz\n",
      "Slide Text  Quiz\n",
      "Which of the following is a challenge associated with ingesting complex data formats into a data processing system?\n",
      "\n",
      "The large size of individual data files.\n",
      "Standardizing naming conventions across data sets.\n",
      "Ensuring the semantic meaning of data is preserved during ingestion.\n",
      "Frequency of data updates.\n",
      "\n",
      "Answer: C\n",
      "\n",
      "Explanation:\n",
      "While data formats can be intricate and varied, the primary challenge is ensuring the actual meaning (semantics) of the data is retained during the ingestion process. \n",
      "Slide Index  2140758000\n",
      "Slide Layout  RunningMan-Infographic\n",
      "Slide Title  In a nutshell, we learnt\n",
      "Slide Text  In a nutshell, we learnt\n",
      "\n",
      "Challenges in Data Ingestion: Understanding the multifaceted issues that arise while ingesting data, ranging from data volume, format intricacies, to security and maintenance.\n",
      "Highlighting the key criteria to consider when choosing data ingestion tools. It's not just about the tool's functionality, but also its scalability, cost, and support.\n",
      "Introducing a variety of tools available in the market - from open-source solutions like Kafka and Spark, cloud offerings like AWS Kinesis, to commercial platforms such as Informatica.\n",
      "Slide Index  329\n",
      "Slide Layout  Section Header\n",
      "No Title\n",
      "Slide Text  Key Points to remember\n",
      "References\n",
      "Summary\n",
      "Slide Index  355\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Key Points to remember\n",
      "Slide Text  Key Points to remember\n",
      "\n",
      "Big Data Framework: Comprises Storage (e.g., HDFS), Processing (e.g., Spark), Analysis (e.g., Hive), and Visualization (e.g., Tableau) components.\n",
      "Data Origin: Big data sources span from web & social media to IoT and business systems.\n",
      "Data Ingestion: The essential process of acquiring, validating, and preparing data from diverse sources.\n",
      "Batch vs Stream: Batch processes accumulated data, while Stream deals with continuous, real-time data flows.\n",
      "Ingestion Pipeline Steps: Include data extraction, validation, transformation, loading, and metadata management.\n",
      "ETL vs ELT: ETL transforms before loading, often slower. ELT loads raw data first, providing flexibility and speed, especially for vast data.\n",
      "Pipeline Considerations: Key factors encompass data sources, transformation requirements, target systems, and latency.\n",
      "Applications: E.g., E-commerce companies like Flipkart utilize pipelines for insights and recommendations, while banks like HDFC focus on real-time data for fraud detection.\n",
      "Challenges: From handling the three V's (volume, variety, velocity) to addressing hardware constraints and ensuring data security.\n",
      "Tool Selection: Considerations include supported data formats, batch/stream capabilities, scalability, and cost.\n",
      "Popular Tools: Open-source tools like Kafka, cloud solutions like AWS Kinesis, and commercial platforms such as Informatica offer diverse options.\n",
      "Slide Index  2140757964\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  References\n",
      "Slide Text  References\n",
      "Books\n",
      "Agile Data Warehouse Design: Collaborative Dimensional Modeling, from Whiteboard to Star Schema by Lawrence CorrBig Data: Using SMART Big Data, Analytics and Metrics To Make Better Decisions and Improve Performance 1st Edition by Bernard Marr \n",
      "Big Data: Principles and best practices of scalable real-time data systems by Nathan Marz\n",
      "Kafka – The Definitive Guide: Real-time data and stream processing at scale Paperback – by Neha Narkhede (Author), Gwen Shapira (Author), Todd Palino (Author)\n",
      "\n",
      "\n",
      "Links\n",
      "https://learn.microsoft.com/en-us/azure/architecture/guide/architecture-styles/big-data\n",
      "https://streamsets.com/learn/data-ingestion/\n",
      "https://www.ibm.com/blog/elt-vs-etl-whats-the-difference/\n",
      "https://www.fivetran.com/learn/data-ingestion\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Slide Index  2140757965\n",
      "Slide Layout  1_Title+Content\n",
      "Slide Title  Thank You\n",
      "Slide Text  Thank You\n"
     ]
    }
   ],
   "source": [
    "for slide in ppt_client.slides:\n",
    "    print(\"Slide Index \",slide.slide_id)\n",
    "    print(\"Slide Layout \",slide.slide_layout.name)\n",
    "    try:\n",
    "        print(\"Slide Title \",slide.shapes.title.text)\n",
    "    except:\n",
    "        print(\"No Title\")\n",
    "    print(\"Slide Text \",ppt_client.get_slide_text(slide))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
